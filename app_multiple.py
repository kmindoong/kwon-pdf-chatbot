import os
import json
import boto3
import streamlit as st
import logging
import base64
from dotenv import load_dotenv
from requests_aws4auth import AWS4Auth
from opensearchpy import OpenSearch, RequestsHttpConnection
from opensearchpy.helpers import bulk
from unstructured.partition.pdf import partition_pdf
from io import BytesIO
from opensearchpy.exceptions import NotFoundError
from streamlit_pdf_viewer import pdf_viewer

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)

# ========================
# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ë° í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
# ========================
load_dotenv()

aws_access_key_id = os.getenv("AWS_ACCESS_KEY_ID")
aws_secret_access_key = os.getenv("AWS_SECRET_ACCESS_KEY")
aws_region = os.getenv("AWS_REGION")
s3_bucket_name = os.getenv("S3_BUCKET_NAME")
opensearch_host = os.getenv("OPENSEARCH_COLLECTION_HOST")
opensearch_index_name = os.getenv("OPENSEARCH_INDEX_NAME")
bedrock_model_id = os.getenv("BEDROCK_MODEL_ID")

if not all([aws_access_key_id, aws_secret_access_key, aws_region, s3_bucket_name, opensearch_host, opensearch_index_name, bedrock_model_id]):
    st.error("`.env` íŒŒì¼ì— í•„ìš”í•œ í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ëª¨ë“  ë³€ìˆ˜ë¥¼ ì±„ì›Œì£¼ì„¸ìš”.")
    st.stop()

session = boto3.Session(
    aws_access_key_id=aws_access_key_id,
    aws_secret_access_key=aws_secret_access_key,
    region_name=aws_region
)
credentials = session.get_credentials()
awsauth = AWS4Auth(
    credentials.access_key,
    credentials.secret_key,
    aws_region,
    'aoss'
)

s3_client = session.client('s3')
bedrock_client = session.client('bedrock-runtime')
opensearch_client = OpenSearch(
    hosts=[{'host': opensearch_host, 'port': 443}],
    http_auth=awsauth,
    use_ssl=True,
    verify_certs=True,
    connection_class=RequestsHttpConnection,
    timeout=30
)

# PDF ë° ì´ë¯¸ì§€ ì €ì¥ í´ë” ì„¤ì • ë° ìƒì„±
PDF_DIR = "./pdf-files"
FIGURES_DIR = "./figures"
os.makedirs(PDF_DIR, exist_ok=True)
os.makedirs(FIGURES_DIR, exist_ok=True)

# ========================
# ì•± ì‹œì‘ ì‹œ PDF íŒŒì¼ ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œ
# ========================
pdf_file_name = "accu.pdf"
local_pdf_path = os.path.join(PDF_DIR, pdf_file_name)

if not os.path.exists(local_pdf_path) or os.path.getsize(local_pdf_path) == 0:
    st.info("PDF íŒŒì¼ì„ S3ì—ì„œ ë‹¤ìš´ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")
    try:
        s3_client.download_file(s3_bucket_name, pdf_file_name, local_pdf_path)
        st.success("PDF íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ. ì•±ì„ ë‹¤ì‹œ ì‹œì‘í•©ë‹ˆë‹¤.")
        st.rerun()
    except Exception as e:
        st.error(f"S3ì—ì„œ PDF íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.stop()


# ========================
# 2. ì¸ë±ì‹± í•¨ìˆ˜ (í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€)
# ========================
def process_pdf_and_index_opensearch_with_images(s3_file_name):
    try:
        logging.info("S3ì—ì„œ PDF íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘...")
        s3_object = s3_client.get_object(Bucket=s3_bucket_name, Key=s3_file_name)
        pdf_content = s3_object['Body'].read()
        logging.info("ë‹¤ìš´ë¡œë“œ ì™„ë£Œ.")

        logging.info("unstructuredë¥¼ ì‚¬ìš©í•˜ì—¬ PDF ë‚´ìš© ë¶„í•  ì¤‘...")
        elements = partition_pdf(
            file=BytesIO(pdf_content),
            strategy="hi_res",
            infer_table_structure=False,
            languages=["kor", "eng"],
            extract_images_in_pdf=True,
            image_output_dir_path="./figures"
        )
        logging.info(f"unstructured ë¶„í•  ì™„ë£Œ. ì¶”ì¶œëœ ìš”ì†Œ ìˆ˜: {len(elements)}")

        pages = {}
        for el in elements:
            page_num = el.metadata.page_number if hasattr(el.metadata, 'page_number') else None
            if page_num not in pages:
                pages[page_num] = {'text': [], 'images': []}
            
            if el.text:
                pages[page_num]['text'].append(el.text)
            
            if hasattr(el.metadata, 'image_path') and el.metadata.image_path:
                pages[page_num]['images'].append(os.path.basename(el.metadata.image_path))

            
        print(f"OpenSearch ì¸ë±ìŠ¤ '{opensearch_index_name}'ì´(ê°€) ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        
        mapping = {
            "settings": {
                "index.knn": True,
            },
            "mappings": {
                "properties": {
                    "text": {"type": "text"},
                    "embedding": {
                        "type": "knn_vector",
                        "dimension": 1024,
                        "method": {
                            "name": "hnsw",
                            "engine": "nmslib"
                        }
                    },
                    "image_paths": {"type": "keyword"},
                    "page_number": {"type": "long"}
                }
            }
        }
                
        opensearch_client.indices.create(index=opensearch_index_name, body=mapping)
        logging.info(f"OpenSearch ì¸ë±ìŠ¤ '{opensearch_index_name}' ìƒì„± ì™„ë£Œ.")

        docs = []
        for page_num, content in pages.items():
            if not content['text'] and not content['images']:
                continue
            combined_text = "\n".join(content['text'])
            
            bedrock_model_id = 'cohere.embed-multilingual-v3'
            response = bedrock_client.invoke_model(
                modelId=bedrock_model_id,
                body=json.dumps({"texts": [combined_text], "input_type": "search_document"})
            )
            embedding = json.loads(response['body'].read())['embeddings'][0]

            doc = {
                'text': combined_text,
                'source': s3_file_name,
                'page_number': page_num,
                'embedding': embedding,
                'image_paths': content['images']
            }
            docs.append({
                '_index': opensearch_index_name,
                '_source': doc
            })

        # docs ë¦¬ìŠ¤íŠ¸ë¥¼ bulkë¡œ í•œ ë²ˆì— ì¸ë±ì‹±
        if docs:
            try:
                successes, failures = bulk(opensearch_client, docs)
                if failures:
                    logging.error(f"ì¸ë±ì‹± ì‹¤íŒ¨: {failures}")
                else:
                    logging.info(f"PDF ë‚´ìš© ë° ì´ë¯¸ì§€ ì •ë³´ê°€ OpenSearchì— ì„±ê³µì ìœ¼ë¡œ ì¸ë±ì‹±ë˜ì—ˆìŠµë‹ˆë‹¤. ë¬¸ì„œ ìˆ˜: {len(docs)}")
            except Exception as e:
                logging.error(f"ë²Œí¬ ì¸ë±ì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        else:
            logging.warning("ì¸ë±ì‹±í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

        logging.info(f"PDF ë‚´ìš© ë° ì´ë¯¸ì§€ ì •ë³´ê°€ OpenSearchì— ì¸ë±ì‹±ë˜ì—ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        logging.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")

# ========================
# 3. RAG í•¨ìˆ˜ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ + ì´ë¯¸ì§€ ì¶œë ¥)
# ========================
def get_rag_answer_from_bedrock_with_images(query):
    try:
        bedrock_model_id_embedding = 'cohere.embed-multilingual-v3'
        response = bedrock_client.invoke_model(
            modelId=bedrock_model_id_embedding,
            body=json.dumps({"texts": [query], "input_type": "search_query"})
        )
        query_embedding = json.loads(response['body'].read())['embeddings'][0]
        
        search_query = {
            "size": 3,
            "query": {
                "bool": {
                    "should": [
                        {
                            "multi_match": {
                                "query": query,
                                "fields": ["text^2"]
                            }
                        },
                        {
                            "knn": {
                                "embedding": {
                                    "vector": query_embedding,
                                    "k": 3
                                }
                            }
                        }
                    ],
                    "minimum_should_match": 1
                }
            },
            "_source": ["text", "image_paths"]
        }
        
        response = opensearch_client.search(index=opensearch_index_name, body=search_query)
        hits = response['hits']['hits']
        context_docs = []
        image_paths = []
        
        for hit in hits:
            context_docs.append(hit['_source']['text'])
            if 'image_paths' in hit['_source'] and hit['_source']['image_paths']:
                image_paths.extend(hit['_source']['image_paths'])
        
        if not context_docs:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì— ëŒ€í•œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", []

        context = "\n\n".join(context_docs)
        
        llm_model_id = os.getenv("BEDROCK_MODEL_ID")
        
        if 'claude' in llm_model_id:
            prompt = f"""
            ë‹¤ìŒì€ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° ìœ„í•œ ì°¸ê³  ìë£Œì…ë‹ˆë‹¤.
            <ìë£Œ>
            {context}
            </ìë£Œ>
            ë‹¹ì‹ ì€ ìœ ìš©í•œ AI ë¹„ì„œì…ë‹ˆë‹¤. ì œê³µëœ ìë£Œë§Œì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— í•œêµ­ì–´ë¡œ ìƒì„¸í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”. ë§Œì•½ ìë£Œì— ë‹µë³€ì´ ì—†ë‹¤ë©´, "ì£„ì†¡í•©ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œì—ëŠ” ë‹µë³€ì´ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë§í•´ì£¼ì„¸ìš”. ì ˆëŒ€ ìë£Œì— ì—†ëŠ” ì •ë³´ë¥¼ ì„ì˜ë¡œ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.
            ì‚¬ìš©ì ì§ˆë¬¸: {query}
            ë‹µë³€:
            """
            body = json.dumps({"anthropic_version": "bedrock-2023-05-31", "max_tokens": 1000, "messages": [{"role": "user", "content": [{"type": "text", "text": prompt}]}], "temperature": 0.5})
            response_body = bedrock_client.invoke_model(modelId=llm_model_id, body=body, contentType="application/json", accept="application/json")
            llm_answer = json.loads(response_body.get('body').read())['content'][0]['text']
        else:
            prompt = f"""
            You are a helpful AI assistant. Use only the provided context to answer the user's question in Korean. If the answer is not in the context, say "ì£„ì†¡í•©ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œì—ëŠ” ë‹µë³€ì´ ì—†ìŠµë‹ˆë‹¤." Do not make up information.
            Context:
            {context}
            Question: {query}
            Answer:
            """
            body = json.dumps({"inputText": prompt, "textGenerationConfig": {"maxTokenCount": 1000, "stopSequences": [], "temperature": 0.5, "topP": 0.9}})
            response_body = bedrock_client.invoke_model(modelId=llm_model_id, body=body)
            llm_answer = json.loads(response_body.get('body').read())['results'][0]['outputText']

        unique_image_paths = list(set(image_paths))
        return llm_answer, unique_image_paths

    except Exception as e:
        return f"RAG ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}", []

# ========================
# 4. Streamlit ì•± ë¡œì§
# ========================
st.set_page_config(layout="wide")
st.title("S3 ê¸°ë°˜ ì±—ë´‡")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []
if 'text_input_value' not in st.session_state:
    st.session_state['text_input_value'] = ""

if 'pdf_page_number' not in st.session_state:
    st.session_state['pdf_page_number'] = 1

# PDF ë·°ì–´ì™€ ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤ë¥¼ ë‘ ê°œì˜ ì»¬ëŸ¼ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
col1, col2 = st.columns([1, 1])

with col1:
    st.markdown("## PDF ë¯¸ë¦¬ë³´ê¸° ë·°ì–´", unsafe_allow_html=True)
    pdf_path = os.path.join(PDF_DIR, "accu.pdf")
    if os.path.exists(pdf_path):
        pdf_file_data = open(pdf_path, "rb").read()
        
        # í˜ì´ì§€ ë„˜ê¸°ê¸° ë²„íŠ¼ ì¶”ê°€
        page_prev, page_info, page_next = st.columns([1, 1, 1])
        with page_prev:
            if st.button("ì´ì „ í˜ì´ì§€"):
                if st.session_state.pdf_page_number > 1:
                    st.session_state.pdf_page_number -= 1
                    st.rerun()
        with page_info:
            st.markdown(f"**í˜ì´ì§€: {st.session_state.pdf_page_number}**", unsafe_allow_html=True)
        with page_next:
            if st.button("ë‹¤ìŒ í˜ì´ì§€"):
                # PDF íŒŒì¼ì˜ ì´ í˜ì´ì§€ ìˆ˜ë¥¼ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸° ì–´ë ¤ìš°ë¯€ë¡œ, ìµœëŒ€ í˜ì´ì§€ ìˆ˜ë¥¼ ì„ì˜ë¡œ ì„¤ì •
                if st.session_state.pdf_page_number < 17:
                    st.session_state.pdf_page_number += 1
                    st.rerun()
        
        pdf_viewer(
            input=pdf_file_data, 
            pages_to_render=[st.session_state.pdf_page_number]
        )
    else:
        st.warning(f"'{pdf_path}' ê²½ë¡œì—ì„œ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¸ë±ì‹±ì„ ì§„í–‰í•˜ë©´ íŒŒì¼ì´ ìƒì„±ë©ë‹ˆë‹¤.")

with col2:
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ì¸ë±ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    index_exists = True
    try:
        opensearch_client.indices.get(index=opensearch_index_name)
    except NotFoundError:
        index_exists = False
    
    # ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•Šì„ ë•Œë§Œ ì¸ë±ì‹± ë²„íŠ¼ í‘œì‹œ
    if not index_exists:
        st.warning(f"OpenSearch ì¸ë±ìŠ¤ '{opensearch_index_name}'ì´(ê°€) ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. PDF ì¸ë±ì‹±ì„ ë¨¼ì € ì§„í–‰í•´ì•¼ í•©ë‹ˆë‹¤.")
        if st.button("PDF ì¸ë±ì‹± ì‹œì‘"):
            with st.spinner("PDFë¥¼ ì¸ë±ì‹±í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”..."):
                s3_file_key = "accu.pdf"
                process_pdf_and_index_opensearch_with_images(s3_file_key)
                st.success("PDF ì¸ë±ì‹±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ì±—ë´‡ì— ì§ˆë¬¸í•´ ë³´ì„¸ìš”.")
                st.rerun()

    # ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•  ë•Œë§Œ ì±—ë´‡ UI í‘œì‹œ
    if index_exists:
        st.success(f"OpenSearch ì¸ë±ìŠ¤ '{opensearch_index_name}'ì´(ê°€) ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ë°”ë¡œ ì§ˆë¬¸í•´ ë³´ì„¸ìš”.")
        
        st.markdown("---")
        st.markdown("## ìƒ˜í”Œ ì§ˆë¬¸ (í´ë¦­í•˜ê³  ë§¨ ì•„ë˜ ì§ˆë¬¸ ë³´ë‚´ê¸° ë²„íŠ¼ í´ë¦­))")

        sample_questions = [
            "í”„ë¡œì íŠ¸ ê´€ë¦¬ í˜ì´ì§€ì—ì„œ ì–´ë–¤ ì‘ì—…ì„ í•  ìˆ˜ ìˆìŠµë‹ˆê¹Œ?",
            "ì‚¬ìš©ì ì´ê´€ ê¸°ëŠ¥ì€ ì–´ë–¤ ê³„ì •ë§Œ ê°€ëŠ¥í•©ë‹ˆê¹Œ?",
            "í”„ë¡œì íŠ¸ ê´€ë¦¬ì—ì„œ íŠ¹ì • í”„ë¡œì íŠ¸ ìˆ˜ì •ì„ í•˜ëŠ” ë°©ë²•ì€? ìˆœì„œëŒ€ë¡œ ì•Œë ¤ì£¼ì„¸ìš”."
        ]

        # ìƒ˜í”Œ ì§ˆë¬¸ ë²„íŠ¼ì„ í´ë¦­í•˜ë©´ st.session_stateì— ê°’ì„ ì €ì¥
        cols = st.columns(2)
        for i, q in enumerate(sample_questions):
            with cols[i % 2]:
                if st.button(q, key=f"btn_{i}"):
                    st.session_state.text_input_value = q
                    st.rerun()
        st.write("---")
        
        # ì±„íŒ… ê¸°ë¡ì„ í‘œì‹œ
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
                if "images" in message:
                    for image_path in message["images"]:
                        st.image(image_path)

    # í…ìŠ¤íŠ¸ ì…ë ¥ì°½ê³¼ ì „ì†¡ ë²„íŠ¼
    if index_exists:
        with st.form(key='chat_form', clear_on_submit=True):
            submit_button_label = "ì „ì†¡"
            if st.session_state.text_input_value:
                submit_button_label = "ğŸŒŸ ì§ˆë¬¸ ë³´ë‚´ê¸°"
            
            user_input = st.text_input(
                "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...", 
                value=st.session_state.text_input_value, 
                key='user_input_key',
                label_visibility='hidden'
            )
            submit_button = st.form_submit_button(submit_button_label)
        
        if submit_button and user_input:
            st.session_state.text_input_value = "" # ì…ë ¥ í›„ ê°’ ì´ˆê¸°í™”
            st.session_state.messages.append({"role": "user", "content": user_input})
            with st.chat_message("user"):
                st.markdown(user_input)
            
            with st.chat_message("assistant"):
                with st.spinner("ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                    answer, image_paths = get_rag_answer_from_bedrock_with_images(user_input)
                    st.markdown(answer)
                    if image_paths:
                        for image_path in image_paths:
                            st.image(os.path.join(FIGURES_DIR, image_path), caption=os.path.basename(image_path))
            
            st.session_state.messages.append({"role": "assistant", "content": answer, "images": [os.path.join(FIGURES_DIR, p) for p in image_paths]})
            st.rerun()